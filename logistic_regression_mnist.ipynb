{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "colab": {
      "name": "logistic_regression_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarhusain/colab-sandbox/blob/master/logistic_regression_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLhhhV-JJpG",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Example\n",
        "\n",
        "Logistic regression implementation with TensorFlow v2 library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_Q5QF0JJpI",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Dataset Overview\n",
        "\n",
        "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. \n",
        "\n",
        "In this example, each image will be converted to float32, normalized to [0, 1] and flattened to a 1-D array of 784 features (28*28).\n",
        "\n",
        "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "More info: http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar5rFISuJJpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "0072b63a-b510-4d7a-c66d-c516f3e2566b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvb60SaeJJpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset parameters.\n",
        "num_classes = 10 # 0 to 9 digits\n",
        "num_features = 784 # 28*28\n",
        "\n",
        "# Training parameters.\n",
        "learning_rate = 0.01\n",
        "training_steps = 1000\n",
        "batch_size = 256\n",
        "display_step = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXBJMbMLJJpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare MNIST data.\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Convert to float32.\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "# Flatten images to 1-D vector of 784 features (28*28).\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "x_train, x_test = x_train / 255., x_test / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prjYBmzIJJpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag0DlvWLJJpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight of shape [784, 10], the 28*28 image features, and total number of classes.\n",
        "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
        "# Bias of shape [10], the total number of classes.\n",
        "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
        "\n",
        "# Logistic regression (Wx + b).\n",
        "def logistic_regression(x):\n",
        "    # Apply softmax to normalize the logits to a probability distribution.\n",
        "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "# Cross-Entropy loss function.\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    # Encode label to a one hot vector.\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "    # Clip prediction values to avoid log(0) error.\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    print (y_true)\n",
        "    print (y_pred)\n",
        "\n",
        "    # Compute cross-entropy.\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "\n",
        "# Accuracy metric.\n",
        "def accuracy(y_pred, y_true):\n",
        "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# Stochastic gradient descent optimizer.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2TeJcE_JJpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimization process. \n",
        "def run_optimization(x, y):\n",
        "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = cross_entropy(pred, y)\n",
        "\n",
        "    # Compute gradients.\n",
        "    gradients = g.gradient(loss, [W, b])\n",
        "    \n",
        "    # Update W and b following gradients.\n",
        "    optimizer.apply_gradients(zip(gradients, [W, b]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tqBfMjMJJpb",
        "colab_type": "code",
        "outputId": "d3fcac3c-c593-4551-f1f2-0b0d39127b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "# Run training for the given number of steps.\n",
        "print(batch_y.shape)\n",
        "print(batch_x.shape)\n",
        "\n",
        "#for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(1), 1):\n",
        "     # Run the optimization to update W and b values.\n",
        "    run_optimization(batch_x, batch_y)\n",
        "    \n",
        "    if step % display_step == 0:\n",
        "        pred = logistic_regression(batch_x)\n",
        "        loss = cross_entropy(pred, batch_y)\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256,)\n",
            "(256, 784)\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]], shape=(256, 10), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[2.38859956e-03 4.53634362e-04 1.81910843e-01 ... 1.72913615e-02\n",
            "  1.05124593e-01 1.31940749e-02]\n",
            " [4.16537970e-02 3.82192608e-04 1.88885644e-01 ... 2.27138400e-01\n",
            "  4.74546999e-01 8.98396876e-03]\n",
            " [3.76405835e-04 3.42439133e-04 9.37270932e-03 ... 1.78615540e-01\n",
            "  7.81132877e-01 1.18050845e-02]\n",
            " ...\n",
            " [6.96213916e-04 2.32113572e-03 5.02356775e-02 ... 9.04146969e-01\n",
            "  1.74896754e-02 1.24527626e-02]\n",
            " [3.02060526e-02 1.46373059e-03 1.49272056e-02 ... 9.56046488e-03\n",
            "  9.41846222e-02 1.12693163e-03]\n",
            " [2.08706439e-01 9.02919273e-05 1.99554302e-02 ... 9.07762069e-03\n",
            "  6.65985584e-01 1.70589471e-03]], shape=(256, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ExdA08JJpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test model on validation set.\n",
        "pred = logistic_regression(x_test)\n",
        "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcZWKr0JJph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize predictions.\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0vWi7qEJJpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict 5 images from validation set.\n",
        "n_images = 5\n",
        "test_images = x_test[:n_images]\n",
        "predictions = logistic_regression(test_images)\n",
        "\n",
        "# Display image and model prediction.\n",
        "for i in range(n_images):\n",
        "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}